{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhfRwRE61cTK"
   },
   "source": [
    "# Importing important library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3bOmkr61GnN",
    "outputId": "ccc01bd8-8694-4887-b79d-1aa5c366e9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3MB 7.9MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 36.5MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 45.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHiH08C91MMR",
    "outputId": "4ef42f36-d6b6-4336-ce04-9dc6a12f9061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold , train_test_split\n",
    "import transformers \n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ls783132DFd"
   },
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "BLxNsoA-2Amb",
    "outputId": "2247723f-b094-4e40-cb25-333309fae955"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-604d04c9-5e26-4556-baa4-046673a053d5\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-604d04c9-5e26-4556-baa4-046673a053d5\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test.csv to test (1).csv\n",
      "Saving train.csv to train (1).csv\n",
      "your uploaded file test.csv with length of file313984\n",
      "your uploaded file train.csv with length of file3501243\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded=files.upload()\n",
    "for fn in uploaded.keys():\n",
    "  print(\"your uploaded file {name} with length of file{len}\".format(name=fn,len=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOUNiJIw20T1"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ov-FJe-2NTo"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "TH23juDt2-Vn",
    "outputId": "5ac4dea9-8a6f-40a5-bd62-555fed1d10ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  ... sentiment\n",
       "0  cb774db0d1  ...   neutral\n",
       "1  549e992a42  ...  negative\n",
       "2  088c60f138  ...  negative\n",
       "3  9642c003ef  ...  negative\n",
       "4  358bd9e861  ...  negative\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sLybfKXJ36pj",
    "outputId": "e931748f-1ec5-4863-e2b5-1234aa141437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "px-IncSI2stL",
    "outputId": "5ca7a379-7ea5-48ba-8e05-c77eed86e402"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27481</td>\n",
       "      <td>27480</td>\n",
       "      <td>27480</td>\n",
       "      <td>27481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>27481</td>\n",
       "      <td>27480</td>\n",
       "      <td>22463</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>85cae2efd9</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>good</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>11118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            textID                                 text selected_text sentiment\n",
       "count        27481                                27480         27480     27481\n",
       "unique       27481                                27480         22463         3\n",
       "top     85cae2efd9  Funeral ceremony...gloomy friday...          good   neutral\n",
       "freq             1                                    1           199     11118"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57I-e9ED3eAz"
   },
   "outputs": [],
   "source": [
    "train=train.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "tVxiJDtI28pP",
    "outputId": "9c0c2473-534c-4ca9-d335-684074f010df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID  ... length\n",
       "0  cb774db0d1  ...     36\n",
       "1  549e992a42  ...     46\n",
       "2  088c60f138  ...     25\n",
       "3  9642c003ef  ...     31\n",
       "4  358bd9e861  ...     75\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['length'] = train['text'].apply(len)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "v0kcZ_4C3XQ2",
    "outputId": "68f1dab3-58e7-4eb9-d611-1b4bcd8e512a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f88952fbe90>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['length'].plot(bins=20,kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kvQP9sV3mvP"
   },
   "outputs": [],
   "source": [
    "# roberta tokenizera is preferred\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orqMsmUq1wIL"
   },
   "outputs": [],
   "source": [
    "def transformed_text(x):\n",
    "  return tokenizer.encode(\" \"+\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USmKH-rf16yH"
   },
   "outputs": [],
   "source": [
    "train['token_text']=train['text'].apply(transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oY0BBLUt5Yki"
   },
   "outputs": [],
   "source": [
    "train['token_text_len']=train['token_text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "4-DX7dxH8EoC",
    "outputId": "7ee23285-4e7c-4428-dbee-2fdb1fac5dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8895e15690>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['token_text_len'].plot(bins=30,kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muWR9LiD8OSP",
    "outputId": "9523a986-9c83-48d5-df4d-20192031e6fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27481.000000\n",
       "mean        19.878935\n",
       "std          9.520686\n",
       "min          2.000000\n",
       "25%         12.000000\n",
       "50%         19.000000\n",
       "75%         27.000000\n",
       "max         88.000000\n",
       "Name: token_text_len, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['token_text_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET0DhJsA_IJC"
   },
   "source": [
    "# neutral sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SdpPozKA3p5"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "umSBNxuNAasg",
    "outputId": "0673b3b8-81e2-4cb5-a18f-2f4b9479c05a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           text                                                                                               selected_text\n",
       "0                                                                           I`d have responded, if I were going                                                                         I`d have responded, if I were going\n",
       "5                  http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth                http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth\n",
       "7                                                                                                    Soooo high                                                                                                  Soooo high\n",
       "8                                                                                                   Both of you                                                                                                 Both of you\n",
       "10   as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff  as much as i love to be hopeful, i reckon the chances are minimal =P i`m never gonna get my cake and stuff"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['sentiment']=='neutral'][['text','selected_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8igxs_G_P9p"
   },
   "outputs": [],
   "source": [
    "# metric\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "def jac_score(x):\n",
    "  return jaccard(x['text'],x['selected_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdQbfQoy_RBT"
   },
   "outputs": [],
   "source": [
    "neutral_jac=train[train['sentiment']=='neutral'].apply(jac_score,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqAdXFa8oGzW",
    "outputId": "ebe36354-4b5a-429a-a18f-f974fab3fa0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9005216765605325"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(neutral_jac==1)/(len(neutral_jac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "nsJtGeMVo__3",
    "outputId": "be368a67-5bf4-48a5-9091-4e9294b0a0f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Thats it, its the end. Tears for Fears vs Eric Prydz, DJ Hero   http://bit.ly/2Hpbg4</td>\n",
       "      <td>Thats it, its the end. Tears for Fears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>will be back later.  http://plurk.com/p/rp3k7</td>\n",
       "      <td>will be back later.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Miles from you   I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers.</td>\n",
       "      <td>Miles from you   I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Hi  how are you doing ???  *just joined twitter...*</td>\n",
       "      <td>Hi  how are you doing ???  *just joined twitter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>if u have a friendster add me!!!!!!!!!        my email adress      add me  loco_crime_1st.com        add me leave some comment</td>\n",
       "      <td>if u have a friendster add me!!!!!!!!!        my email adress      add me  loco_crime_1st.com        add me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27315</th>\n",
       "      <td>Poor you   If I was with you right now; I would probably give you a hug ;D</td>\n",
       "      <td>Poor you   If I was with you right now; I would probably give you a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27332</th>\n",
       "      <td>haha i see im so bored rite now.. it seems like everyone is headin to vegas this weekend</td>\n",
       "      <td>haha i see im so bored rite now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27349</th>\n",
       "      <td>yea but that`s an old pic,  she looks a lot different now...she turned into such a beautiful women   I miss her A LOT!</td>\n",
       "      <td>yea but that`s an old pic,  she looks a lot different now...she turned into such a beautiful women   I miss her A LO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27374</th>\n",
       "      <td>says Finally, Im home.  http://plurk.com/p/rr121</td>\n",
       "      <td>says Finally, Im home.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs))</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1106 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 text                                                                                                               selected_text\n",
       "35                                               Thats it, its the end. Tears for Fears vs Eric Prydz, DJ Hero   http://bit.ly/2Hpbg4                                                                                      Thats it, its the end. Tears for Fears\n",
       "57                                                                                      will be back later.  http://plurk.com/p/rp3k7                                                                                                         will be back later.\n",
       "86        Miles from you   I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers.  Miles from you   I`m in Essex so give me plenty of warning so I can arrive in time to get at least one of those free beers\n",
       "92                                                                                Hi  how are you doing ???  *just joined twitter...*                                                                          Hi  how are you doing ???  *just joined twitter...\n",
       "110    if u have a friendster add me!!!!!!!!!        my email adress      add me  loco_crime_1st.com        add me leave some comment                 if u have a friendster add me!!!!!!!!!        my email adress      add me  loco_crime_1st.com        add me\n",
       "...                                                                                                                               ...                                                                                                                         ...\n",
       "27315                                                      Poor you   If I was with you right now; I would probably give you a hug ;D                                                     Poor you   If I was with you right now; I would probably give you a hug\n",
       "27332                                        haha i see im so bored rite now.. it seems like everyone is headin to vegas this weekend                                                                                            haha i see im so bored rite now.\n",
       "27349          yea but that`s an old pic,  she looks a lot different now...she turned into such a beautiful women   I miss her A LOT!        yea but that`s an old pic,  she looks a lot different now...she turned into such a beautiful women   I miss her A LO\n",
       "27374                                                                                says Finally, Im home.  http://plurk.com/p/rr121                                                                                                      says Finally, Im home.\n",
       "27480                                                                     All this flirting going on - The ATG smiles. Yay.  ((hugs))                                                                  All this flirting going on - The ATG smiles. Yay.  ((hugs)\n",
       "\n",
       "[1106 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['sentiment']=='neutral'][neutral_jac!=1][['text','selected_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWZUf8fPABNr",
    "outputId": "8a48fa0d-650f-4955-fa3f-d8abb3ed12d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976401816076681"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_jac.mean() # neutral sentiment row has selected_text same as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecjLqCD6CSvv"
   },
   "source": [
    "# Dataset labeling issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XinaEK79Ct5J"
   },
   "outputs": [],
   "source": [
    "valid=[]\n",
    "invalid=[]\n",
    "for k in range(len(train)):\n",
    "  tex=tokenizer.encode(\" \"+\" \".join(train.loc[k,'text'].split()))\n",
    "  sel=tokenizer.encode(\" \"+\" \".join(train.loc[k,'selected_text'].split()))\n",
    "  b=sel[1:-1]\n",
    "  a=tex[1:-1]\n",
    "  t=[x for x in range(len(a)) if a[x:x+len(b)] == b]\n",
    "  if t!=[]:\n",
    "    valid.append(k);\n",
    "  else:\n",
    "    invalid.append(k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R82fMFcSHWG",
    "outputId": "d9aa04d7-d2f4-42a2-85be-75b166757522"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24891"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "lXchPaAtC9X0",
    "outputId": "99c95552-6c7c-4f2c-f21b-43f595e91d60"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "      <td>onna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>On the way to Malaysia...no internet access to Twit</td>\n",
       "      <td>.no internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>If it is any consolation I got my BMI tested hahaha it says I am obesed  well so much for being unhappy for about 10 minutes.</td>\n",
       "      <td>well so much for being unhappy for about 10 minute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>A little happy for the wine jeje ok it`sm my free time so who cares, jaja i love this day</td>\n",
       "      <td>A little happy fo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>i donbt like to peel prawns, i also dont like going shopping, running out of money and crawling round the car looking for more</td>\n",
       "      <td>dont like go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>which case? I got a new one last week and I`m not thrilled at all with mine.</td>\n",
       "      <td>d I`m not thrilled at all with mine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mannnn..... _ got an iphone!!! im jealous....  http://bit.ly/NgnaR</td>\n",
       "      <td>jealous..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>He`s awesome... Have you worked with him before? He`s a good friend.</td>\n",
       "      <td>s awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Still no reply from  about my SimFinger problem  So no iRape parody video until I get a response, sorry guys</td>\n",
       "      <td>, sorry guys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1 week post my'horrible, traumatic jumping cholla accident.'-cholla`s next dirty trick:pieces are starting to emerge from my hand! Ouch!</td>\n",
       "      <td>horrible,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text                                       selected_text\n",
       "18                                                                                                 is back home now      gonna miss every one                                                onna\n",
       "27                                                                                        On the way to Malaysia...no internet access to Twit                                        .no internet\n",
       "32              If it is any consolation I got my BMI tested hahaha it says I am obesed  well so much for being unhappy for about 10 minutes.  well so much for being unhappy for about 10 minute\n",
       "39                                                  A little happy for the wine jeje ok it`sm my free time so who cares, jaja i love this day                                   A little happy fo\n",
       "48             i donbt like to peel prawns, i also dont like going shopping, running out of money and crawling round the car looking for more                                        dont like go\n",
       "49                                                               which case? I got a new one last week and I`m not thrilled at all with mine.                d I`m not thrilled at all with mine.\n",
       "64                                                                         mannnn..... _ got an iphone!!! im jealous....  http://bit.ly/NgnaR                                           jealous..\n",
       "66                                                                       He`s awesome... Have you worked with him before? He`s a good friend.                                           s awesome\n",
       "84                               Still no reply from  about my SimFinger problem  So no iRape parody video until I get a response, sorry guys                                        , sorry guys\n",
       "102  1 week post my'horrible, traumatic jumping cholla accident.'-cholla`s next dirty trick:pieces are starting to emerge from my hand! Ouch!                                           horrible,"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[invalid[:10],['text','selected_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTKhtwFxrHpP"
   },
   "source": [
    "# try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rg_CF5KOs7nL",
    "outputId": "6f9d1143-ca3b-401d-f8cc-60b17a11b585"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'how are you'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_test=\" \".join(\" how are you \".split())\n",
    "try_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Lfr59bOIwDnJ",
    "outputId": "82ac2c5a-ab7a-4ad0-814d-b12b4de22afd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' how'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([141])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RsxJD-FuwK9p",
    "outputId": "baffc07b-2991-467c-a95f-d6c092347daf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' gonna'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([6908])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkuBoS4HtmFM",
    "outputId": "cfa02e20-d033-4e53-9522-bcb7601b7b43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9178, 32, 47, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(try_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNKUFA40txOP",
    "outputId": "dadcd4b6-09df-49de-8b29-7d932cace7c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 141, 32, 47, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" \"+try_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uez5yCMirMnd",
    "outputId": "d929fedc-b7eb-465b-da85-8b18b584ebb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' what interview! leave me alone', 'leave me alone')\n",
      "31 14\n",
      "17\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0, 99, 1194, 328, 989, 162, 1937, 2]\n",
      "<s> 3\n",
      " what 5\n",
      " interview 10\n",
      "! 1\n",
      " leave 6\n",
      " me 3\n",
      " alone 6\n",
      "</s> 4\n",
      "38\n",
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "k=3\n",
    "tex =\" \"+\" \".join(train.loc[k,'text'].split()) # removing spaces \n",
    "sel= \" \".join(train.loc[k,'selected_text'].split()) # need not be space at first place\n",
    "print((tex,sel))\n",
    "print(len(tex),len(sel))\n",
    "idx = tex.find(sel)\n",
    "print(idx)\n",
    "chars = np.zeros((len(tex)))\n",
    "chars[idx:idx+len(sel)]=1\n",
    "print(chars)\n",
    "enc_t = tokenizer.encode(tex)\n",
    "print(enc_t)\n",
    "enc_len=0\n",
    "for t in enc_t:\n",
    "  print(tokenizer.decode([t]),len(tokenizer.decode([t])))\n",
    "  enc_len+=len(tokenizer.decode([t]))\n",
    "print(enc_len)\n",
    "idx=0\n",
    "tok_idx=[]\n",
    "for i in range(1,len(enc_t)-1):\n",
    "  wid=len(tokenizer.decode([enc_t[i]]))\n",
    "  if np.sum(chars[idx:idx+wid])>0:\n",
    "     tok_idx.append(i)\n",
    "  idx+=wid\n",
    "print(tok_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IGfDFghBOcG"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8whLJzhVB-aT",
    "outputId": "97074abf-b0ff-4853-be7e-0a499e78bf44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "\t[0, 2430, 1313, 7974, 2]\n",
      "attention_mask:\n",
      "\t[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# token of \" positive negative neutral\"\n",
    "tokens_pt = tokenizer(\" negative positive neutral\")\n",
    "for key, value in tokens_pt.items():\n",
    "    print(\"{}:\\n\\t{}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4s1UXanCrWq"
   },
   "outputs": [],
   "source": [
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyGbi-YYtf48"
   },
   "outputs": [],
   "source": [
    "MAX_LEN=91\n",
    "ct = train.shape[0]\n",
    "\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32') # start of output\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')  # end of output\n",
    "\n",
    "for k in range(ct):\n",
    "  # find overlap\n",
    "  tex =\" \"+\" \".join(train.loc[k,'text'].split()) # removing spaces \n",
    "  sel= \" \".join(train.loc[k,'selected_text'].split()) # need not be space at first place\n",
    "  idx = tex.find(sel)\n",
    "\n",
    "  # making bolean\n",
    "  chars = np.zeros((len(tex)))\n",
    "  chars[idx:idx+len(sel)]=1\n",
    "\n",
    "  # find position \n",
    "\n",
    "  enc_t = tokenizer.encode(tex)\n",
    "  idx=0\n",
    "  tok_idx=[]\n",
    "  for i in range(1,len(enc_t)-1):\n",
    "    wid=len(tokenizer.decode([enc_t[i]]))\n",
    "    if np.sum(chars[idx:idx+wid])>0:\n",
    "      tok_idx.append(i)\n",
    "    idx+=wid\n",
    "  input_ids[k,:len(enc_t)+3]=enc_t+[2]+[sentiment_id[train.loc[k,'sentiment']]]+[2]\n",
    "  attention_mask[k,:len(enc_t)+3]=1\n",
    "  if tok_idx!=[]:\n",
    "    start_tokens[k,tok_idx[0]]=1\n",
    "    end_tokens[k,tok_idx[-1]+1]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHwt9gD8uWMK"
   },
   "source": [
    "Alternate data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPv7Gpx4LJT0"
   },
   "outputs": [],
   "source": [
    "#eliminating all invalid label data\n",
    "input_ids_test = np.ones((ct,MAX_LEN),dtype='int32') # 1 is for padding \n",
    "attention_mask_test = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens_test = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens_test = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "for k in valid:\n",
    "  tex=tokenizer.encode(\" \"+\" \".join(train.loc[k,'text'].split()))\n",
    "  sel=tokenizer.encode(\" \"+\" \".join(train.loc[k,'selected_text'].split()))\n",
    "  b=sel[1:-1]\n",
    "  a=tex[1:-1]\n",
    "  t=[x for x in range(len(a)) if a[x:x+len(b)] == b]\n",
    "  input_ids_test[k,:len(tex)+3]=tex+[2]+[sentiment_id[train.loc[k,'sentiment']]]+[2]\n",
    "  attention_mask_test[k,:len(tex)+3]=1\n",
    "  start_tokens_test[k,1+t[0]]=1\n",
    "  end_tokens_test[k,t[0]+len(b)+1]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsj-jiX2DO2N"
   },
   "source": [
    "# Testing data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6wtpqw_DDSk"
   },
   "outputs": [],
   "source": [
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "for k in range(test.shape[0]):\n",
    "    # INPUT_IDS\n",
    "    tex = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc_t= tokenizer.encode(tex)                \n",
    "    input_ids_t[k,:len(enc_t)+3] = enc_t + [2] + [sentiment_id[test.loc[k,'sentiment']]] + [2]\n",
    "    attention_mask_t[k,:len(enc_t)+3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sVtfuyUDlR8"
   },
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyZTXSb7DfGY"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    bert_model = transformers.TFRobertaModel.from_pretrained('roberta-base')\n",
    "    x = bert_model(ids,attention_mask=att)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXlu26o9Do0j"
   },
   "outputs": [],
   "source": [
    "# metric\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ3sZlLwKYy8"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvJQ2fHgDxHf",
    "outputId": "61cfef88-cd01-4bbc-fed5-4144ac9d3064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "### FOLD 1\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8933b9ade0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8933b9ade0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f894f443d40> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7f894f443d40> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "687/687 [==============================] - ETA: 0s - loss: 3.1581 - activation_loss: 1.4610 - activation_1_loss: 1.6972WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "687/687 [==============================] - 467s 625ms/step - loss: 3.1571 - activation_loss: 1.4605 - activation_1_loss: 1.6965 - val_loss: 1.7485 - val_activation_loss: 0.8627 - val_activation_1_loss: 0.8858\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74845, saving model to v0-roberta-0.h5\n",
      "Epoch 2/2\n",
      "687/687 [==============================] - 427s 622ms/step - loss: 1.7334 - activation_loss: 0.8835 - activation_1_loss: 0.8499 - val_loss: 1.6814 - val_activation_loss: 0.8425 - val_activation_1_loss: 0.8389\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74845 to 1.68137, saving model to v0-roberta-0.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "172/172 [==============================] - 34s 185ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 21s 186ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.7012093469421958\n",
      "\n",
      "####################\n",
      "### FOLD 2\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "688/688 [==============================] - ETA: 0s - loss: 2.9992 - activation_loss: 1.3877 - activation_1_loss: 1.6115WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "688/688 [==============================] - 452s 637ms/step - loss: 2.9981 - activation_loss: 1.3872 - activation_1_loss: 1.6109 - val_loss: 1.7410 - val_activation_loss: 0.8990 - val_activation_1_loss: 0.8420\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.74101, saving model to v0-roberta-1.h5\n",
      "Epoch 2/2\n",
      "688/688 [==============================] - 436s 633ms/step - loss: 1.7021 - activation_loss: 0.8605 - activation_1_loss: 0.8415 - val_loss: 1.7001 - val_activation_loss: 0.8541 - val_activation_1_loss: 0.8460\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.74101 to 1.70014, saving model to v0-roberta-1.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "172/172 [==============================] - 34s 186ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 21s 186ms/step\n",
      ">>>> FOLD 2 Jaccard = 0.6940741609826094\n",
      "\n",
      "####################\n",
      "### FOLD 3\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "688/688 [==============================] - ETA: 0s - loss: 2.8732 - activation_loss: 1.4087 - activation_1_loss: 1.4645WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "688/688 [==============================] - 452s 635ms/step - loss: 2.8723 - activation_loss: 1.4083 - activation_1_loss: 1.4640 - val_loss: 1.6744 - val_activation_loss: 0.8589 - val_activation_1_loss: 0.8154\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.67437, saving model to v0-roberta-2.h5\n",
      "Epoch 2/2\n",
      "688/688 [==============================] - 435s 632ms/step - loss: 1.7273 - activation_loss: 0.8753 - activation_1_loss: 0.8520 - val_loss: 1.7113 - val_activation_loss: 0.8919 - val_activation_1_loss: 0.8194\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.67437\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "172/172 [==============================] - 34s 186ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 21s 185ms/step\n",
      ">>>> FOLD 3 Jaccard = 0.6996163624254944\n",
      "\n",
      "####################\n",
      "### FOLD 4\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "688/688 [==============================] - ETA: 0s - loss: 2.8465 - activation_loss: 1.3887 - activation_1_loss: 1.4578WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "688/688 [==============================] - 452s 636ms/step - loss: 2.8455 - activation_loss: 1.3883 - activation_1_loss: 1.4572 - val_loss: 1.7076 - val_activation_loss: 0.8649 - val_activation_1_loss: 0.8427\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70761, saving model to v0-roberta-3.h5\n",
      "Epoch 2/2\n",
      "688/688 [==============================] - 435s 633ms/step - loss: 1.6591 - activation_loss: 0.8458 - activation_1_loss: 0.8133 - val_loss: 1.6524 - val_activation_loss: 0.8411 - val_activation_1_loss: 0.8113\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70761 to 1.65239, saving model to v0-roberta-3.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "172/172 [==============================] - 34s 185ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 21s 185ms/step\n",
      ">>>> FOLD 4 Jaccard = 0.6992473335637099\n",
      "\n",
      "####################\n",
      "### FOLD 5\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "688/688 [==============================] - ETA: 0s - loss: 2.8710 - activation_loss: 1.4066 - activation_1_loss: 1.4643WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "688/688 [==============================] - 452s 636ms/step - loss: 2.8700 - activation_loss: 1.4062 - activation_1_loss: 1.4638 - val_loss: 1.7029 - val_activation_loss: 0.8812 - val_activation_1_loss: 0.8218\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.70293, saving model to v0-roberta-4.h5\n",
      "Epoch 2/2\n",
      "688/688 [==============================] - 435s 632ms/step - loss: 1.6707 - activation_loss: 0.8588 - activation_1_loss: 0.8119 - val_loss: 1.6565 - val_activation_loss: 0.8577 - val_activation_1_loss: 0.7988\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.70293 to 1.65654, saving model to v0-roberta-4.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "172/172 [==============================] - 35s 186ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 21s 186ms/step\n",
      ">>>> FOLD 5 Jaccard = 0.6927131910720619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*20)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*20)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=2, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>=b: \n",
    "            st = train.loc[k,'text'] \n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc[a:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPEiu2PIIsWo"
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>=b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc[a:b])\n",
    "    result.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854
    },
    "id": "9CnvKArZdFvk",
    "outputId": "837be021-3867-48a0-a63e-2ebd79af8c90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>ed4b3d03eb</td>\n",
       "      <td>I took my antibiotics and I still feel like ****</td>\n",
       "      <td>negative</td>\n",
       "      <td>****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>de20a4f9ba</td>\n",
       "      <td>What happened with her??? Why is she being so wierd all of a sudden???</td>\n",
       "      <td>negative</td>\n",
       "      <td>wierd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>d64eec0d6c</td>\n",
       "      <td>Yes I think so unless it changed.  I sent u a happy mothers day text but u never responded back</td>\n",
       "      <td>negative</td>\n",
       "      <td>u never responded back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>ceab14b717</td>\n",
       "      <td>Heading to the office through a very quiet Stockholm, at least its not sunny</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Heading to the office through a very quiet Stockholm, at least its not sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1620bd719e</td>\n",
       "      <td>All done. No more internship.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>All done. No more internship.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>363460c8f4</td>\n",
       "      <td>Just got to the gym but w/no energy</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Just got to the gym but w/no energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>932227184c</td>\n",
       "      <td>At every gas station, I look for postcards for   No luck.</td>\n",
       "      <td>negative</td>\n",
       "      <td>No luck.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>49e9fe8b96</td>\n",
       "      <td>i want to go to singapore but my mother seems not</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i want to go to singapore but my mother seems not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>683149f529</td>\n",
       "      <td>well maybe someone will care</td>\n",
       "      <td>positive</td>\n",
       "      <td>well maybe someone will care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>9ddcd3f037</td>\n",
       "      <td>http://twitpic.com/4jco5 - The Result of working the Green/Black fingers...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Result of working the Green/Black fingers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>7b2a38faa7</td>\n",
       "      <td>Need to get an adjustment, neck is all out of whack</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Need to get an adjustment, neck is all out of whack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509</th>\n",
       "      <td>b9b4e92118</td>\n",
       "      <td>Back at work</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Back at work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>1dbae05df7</td>\n",
       "      <td>I`m glad you`re little Prissy is doing well. It`s obvious how much you love her w/the treatment she`s getting</td>\n",
       "      <td>positive</td>\n",
       "      <td>I`m glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>cccb3d7081</td>\n",
       "      <td>will do</td>\n",
       "      <td>neutral</td>\n",
       "      <td>will do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>5de3c34293</td>\n",
       "      <td>i am the only arabic girl who`s online  every one is  a sleep ..</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i am the only arabic girl who`s online every one is a sleep..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3279</th>\n",
       "      <td>d205b332e6</td>\n",
       "      <td>are you serious!? that bloooooows</td>\n",
       "      <td>negative</td>\n",
       "      <td>that bloooooows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>52e7b6c3d1</td>\n",
       "      <td>moving offices  I`m going to miss you Hollywood.</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>65e715d6e5</td>\n",
       "      <td>I`ve been there.  The only place I have flown out of since moving up north. Really pretty area for flying, but very expensive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`ve been there. The only place I have flown out of since moving up north. Really pretty area for flying, but very expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>06ed7a838d</td>\n",
       "      <td>got the twilight board game today  good old ebay..</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>ecc56a776a</td>\n",
       "      <td>most definitely will!  also, i wanted to say 'perfect time' was my favorite track! it seriously made me tear up!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>most definitely will! also, i wanted to say 'perfect time' was my favorite track! it seriously made me tear up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>1305139c4a</td>\n",
       "      <td>_kikireestl nooo. you were on my yahoo account. hmm. i wanna say b2k days. idk fo`sho</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_kikireestl nooo. you were on my yahoo account. hmm. i wanna say b2k days. idk fo`sho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>cb20977501</td>\n",
       "      <td>movie then sleep! Today was good day  [{H!--D3ff}]</td>\n",
       "      <td>positive</td>\n",
       "      <td>good day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3066</th>\n",
       "      <td>e96c25a560</td>\n",
       "      <td>I wish I could twitter from the BOOK EXPO but the reception at the Javitz is horrible....</td>\n",
       "      <td>negative</td>\n",
       "      <td>horrible....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>689c6b69c9</td>\n",
       "      <td>I wash my dishes Like a boss!! LOL</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I wash my dishes Like a boss!! LOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>c5a584d398</td>\n",
       "      <td>charlie and the chocolate factory, in the mood for some johnny depp, then bed. Preparing for a 14 1/2 hour wok day tomorrow</td>\n",
       "      <td>neutral</td>\n",
       "      <td>charlie and the chocolate factory, in the mood for some johnny depp, then bed. Preparing for a 14 1/2 hour wok day tomorrow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID  ...                                                                                                                  selected_text\n",
       "1869  ed4b3d03eb  ...                                                                                                                           ****\n",
       "2457  de20a4f9ba  ...                                                                                                                          wierd\n",
       "2014  d64eec0d6c  ...                                                                                                         u never responded back\n",
       "2458  ceab14b717  ...                                                   Heading to the office through a very quiet Stockholm, at least its not sunny\n",
       "689   1620bd719e  ...                                                                                                  All done. No more internship.\n",
       "1817  363460c8f4  ...                                                                                            Just got to the gym but w/no energy\n",
       "3169  932227184c  ...                                                                                                                       No luck.\n",
       "371   49e9fe8b96  ...                                                                              i want to go to singapore but my mother seems not\n",
       "1488  683149f529  ...                                                                                                   well maybe someone will care\n",
       "2913  9ddcd3f037  ...                                                                               The Result of working the Green/Black fingers...\n",
       "3328  7b2a38faa7  ...                                                                            Need to get an adjustment, neck is all out of whack\n",
       "2509  b9b4e92118  ...                                                                                                                   Back at work\n",
       "2521  1dbae05df7  ...                                                                                                                       I`m glad\n",
       "1327  cccb3d7081  ...                                                                                                                        will do\n",
       "1862  5de3c34293  ...                                                                  i am the only arabic girl who`s online every one is a sleep..\n",
       "3279  d205b332e6  ...                                                                                                                that bloooooows\n",
       "2771  52e7b6c3d1  ...                                                                                                                           miss\n",
       "1216  65e715d6e5  ...   I`ve been there. The only place I have flown out of since moving up north. Really pretty area for flying, but very expensive\n",
       "2837  06ed7a838d  ...                                                                                                                           good\n",
       "2649  ecc56a776a  ...                most definitely will! also, i wanted to say 'perfect time' was my favorite track! it seriously made me tear up!\n",
       "3351  1305139c4a  ...                                          _kikireestl nooo. you were on my yahoo account. hmm. i wanna say b2k days. idk fo`sho\n",
       "753   cb20977501  ...                                                                                                                       good day\n",
       "3066  e96c25a560  ...                                                                                                                   horrible....\n",
       "3274  689c6b69c9  ...                                                                                             I wash my dishes Like a boss!! LOL\n",
       "490   c5a584d398  ...    charlie and the chocolate factory, in the mood for some johnny depp, then bed. Preparing for a 14 1/2 hour wok day tomorrow\n",
       "\n",
       "[25 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = result\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaYrRetDG3mz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter Sentiment Extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
